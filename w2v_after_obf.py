# -*- coding: utf-8 -*-
"""W2V after Obf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s6kkcuQNuEZSrSJ8EEkVG-uwKE6EAjtW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

sm = pd.read_csv('dt_sm.csv')
md = pd.read_csv('dt_md.csv')
lg = pd.read_csv('dt_lg.csv')

sm

md

lg

import multiprocessing
cores = multiprocessing.cpu_count()

from gensim.models import Word2Vec
import random

from pyparsing import WordStart

class MySentences(object): #It splits each sentence into words and yields them as separate tokens. The class also implements methods for length retrieval and item retrieval, which allows treating an instance of this class as a list-like object.
    def __init__(self, text):
        self.text = text

    def __iter__(self):
        for line in self.text:
            yield line.split()

    def __len__(self):
        return len(self.text)

    def __getitem__(self, i):
        return self.text[i]

    def __setitem__(self, index, item1):
        self.text[index] = item1

class W2V(object):
    def __init__(self, sentences, cores, vec_size) -> None:
        self.sentences = sentences
        self.model = Word2Vec(min_count=10,
                     window=10,
                     vector_size=vec_size,
                     sample=6e-5,
                     alpha=0.03,
                     min_alpha=0.0007,
                     negative=20,
                     workers=cores-1)

    def train(self):
        self.model.build_vocab(self.sentences)
        self.model.train(self.sentences, total_examples=self.model.corpus_count, epochs=30, report_delay=1)

    def get_similar_word(self, keywords, start_radius=0, end_radius=100): # Returns a random similar word for each keyword within the specified radius range. It queries the model for the most similar words to each keyword.
        word_list = [self.model.wv.most_similar(keywords[i], topn=end_radius)[start_radius:] for i in range(len(keywords))]
        return [random.choice(words)[0] for words in word_list]

    def get_similar_word_2(self, keywords, start_radius=0, end_radius=100): # it returns a list of lists of similar words for each keyword. It doesn't choose a random word from each list.
        #print(len(keywords))
        #print(self.model.wv.most_similar(keywords[i], topn=end_radius))
        word_list = [self.model.wv.most_similar(keywords[i], topn=end_radius)[start_radius:] for i in range(len(keywords))]
        return word_list


    def get_similar_word_list(self, keywords, start_radius = 0, end_radius = 100):
        if isinstance(keywords, list):
            TypeError("keywords should be a list")
        else:
            keywords = [keywords]
            for keyword in keywords:
                word_list = self.model.wv.most_similar(keyword, topn = end_radius)
        return word_list


    def save(self, route): #Saves the trained Word2Vec model
        self.model.save(route)

    def load(self, route): #loads the trained Word2Vec model
        self.model = Word2Vec.load(route)

    def find_word_idx(self, keywords): #Finds the indices of sentences containing each keyword in the provided list of keywords.
        return [[i for i in range(len(self.sentences)) if keywords[j] in self.sentences[i]] for j in range(len(keywords))]

    def obfuscate(self, sentence, keywords): #Replaces keywords in a sentence with similar words obtained from the model.
        word_list = self.get_similar_word(keywords=keywords)
        for i in range(len(keywords)):
            sentence = sentence.replace(keywords[i], word_list[i])
        return sentence

sentences = MySentences(sm.TEXT)
sentences

w2v_model = W2V(sentences, 1, 300) # sentences , core, vec_size

w2v_model.train()

w2v_model.save("w2vmodel_sm")

w2v_model.find_word_idx('chest')



# Import the necessary libraries
import pandas as pd
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
# Tokenize the 'sm.Text' column
tokenized_data = sm['TEXT'].apply(word_tokenize)

# Train the Word2Vec model
model = Word2Vec(tokenized_data, vector_size=100, window=5, min_count=1, sg=0)

vector = model.wv['chest']

vector

vector1 = model.wv['breath']

vector1

vector2 = model.wv['long']

vector2

"""## Computing the length for each Clones"""

len(sm['TEXT'][0])

max_len_sm = max(len(item) for item in sm['TEXT'])
min_len_sm = min(len(item) for item in sm['TEXT'])
avg_len_sm = sum(len(item) for item in sm['TEXT']) / len(sm['TEXT'])
print('max_len :',max_len_sm)
print('min_len :',min_len_sm)
print('avg_len:', avg_len_sm)

max_len_md = max(len(item) for item in md['TEXT'])
min_len_md = min(len(item) for item in md['TEXT'])
avg_len_md = sum(len(item) for item in md['TEXT']) / len(md['TEXT'])
print('max_len :',max_len_md)
print('min_len :',min_len_md)
print('avg_len:', avg_len_md)

max_len_lg = max(len(item) for item in lg['TEXT'])
min_len_lg = min(len(item) for item in lg['TEXT'])
avg_len_lg = sum(len(item) for item in lg['TEXT']) / len(lg['TEXT'])
print('max_len :',max_len_lg)
print('min_len :',min_len_lg)
print('avg_len:', avg_len_lg)

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
max_len_index = sm['TEXT'].apply(len).idxmax()
max_len_row = sm.loc[max_len_index]

print('Row with Maximum Length:')
print(max_len_row)

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
min_len_index = sm['TEXT'].apply(len).idxmin()
min_len_row = sm.loc[min_len_index]

print('Row with Minimum Length:')
print(min_len_row)

lg['TEXT'][10928]

lg['TEXT'][3803]

"""## Distribution for the length of each Clones"""

import pandas as pd

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
length_distribution = sm['TEXT'].apply(len).value_counts().sort_index()

# Print the length distribution
print('Length Distribution:')
print(length_distribution)

#Output's left side is length and right side is the # of rows that has that particular length

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
sm['text_length'] = sm['TEXT'].apply(len)

# Create a distribution plot
sns.histplot(sm['text_length'], kde=True)
plt.title('Distribution of Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
sm['text_length'] = sm['TEXT'].apply(len)

# Create a distribution plot with x-axis range from 0 to 5000
sns.histplot(sm['text_length'], kde=True)
plt.title('Distribution of Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.xlim(0, 5000)  # Set the x-axis range
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
sm['text_length'] = sm['TEXT'].apply(len)

# Create a distribution plot with x-axis range from 0 to 5000
sns.histplot(sm['text_length'], kde=True)
plt.title('Distribution of Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.xlim(0, 100)  # Set the x-axis range
plt.ylim(0, 400)  # Set the x-axis range
plt.show()

import pandas as pd

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
length_distribution = md['TEXT'].apply(len).value_counts().sort_index()

# Print the length distribution
print('Length Distribution:')
print(length_distribution)

#Output's left side is length and right side is the # of rows that has that particular length

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
md['text_length'] = md['TEXT'].apply(len)

# Create a distribution plot
sns.histplot(md['text_length'], kde=True)
plt.title('Distribution of Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
sm['text_length'] = sm['TEXT'].apply(len)

# Create a distribution plot with x-axis range from 0 to 5000
sns.histplot(md['text_length'], kde=True)
plt.title('Distribution of Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.xlim(0, 5000)  # Set the x-axis range
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
md['text_length'] = md['TEXT'].apply(len)

# Create a distribution plot with x-axis range from 0 to 5000
sns.histplot(md['text_length'], kde=True)
plt.title('Distribution of Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.xlim(0, 100)  # Set the x-axis range
plt.ylim(0, 400)  # Set the x-axis range
plt.show()

import pandas as pd

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
length_distribution = lg['TEXT'].apply(len).value_counts().sort_index()

# Print the length distribution
print('Length Distribution:')
print(length_distribution)

#Output's left side is length and right side is the # of rows that has that particular length

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
lg['text_length'] = lg['TEXT'].apply(len)

# Create a distribution plot
sns.histplot(lg['text_length'], kde=True)
plt.title('Distribution of Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
lg['text_length'] = lg['TEXT'].apply(len)

# Create a distribution plot with x-axis range from 0 to 5000
sns.histplot(lg['text_length'], kde=True)
plt.title('Distribution of Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.xlim(0, 5000)  # Set the x-axis range
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming sm is your DataFrame and 'TEXT' is the name of your TEXT column
lg['text_length'] = lg['TEXT'].apply(len)

# Create a distribution plot with x-axis range from 0 to 5000
sns.histplot(lg['text_length'], kde=True)
plt.title('Distribution of Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.xlim(0, 100)  # Set the x-axis range
plt.ylim(0, 400)  # Set the x-axis range
plt.show()









"""## Padding by using max_len"""

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(sm['TEXT'])
sm_v = tokenizer.texts_to_sequences(sm['TEXT'])

sm_v[0]

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(md['TEXT'])
md_v = tokenizer.texts_to_sequences(md['TEXT'])

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(lg['TEXT'])
lg_v = tokenizer.texts_to_sequences(lg['TEXT'])



for sentence in sm_v:
    while len(sentence) < max_len: # max_len이 마지막에 구한 걸로 통일돼 있을거야
        sentence.append(0)

padded_sm_max = np.array(sm_v)
padded_sm[0]

for sentence in md_v:
    while len(sentence) < max_len:
        sentence.append(0)

padded_md_max = np.array(md_v)
padded_md[0]

for sentence in lg_v:
    while len(sentence) < max_len:
        sentence.append(0)

padded_lg_max = np.array(lg_v)
padded_lg[0]

"""## Padding by avg_len"""

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(sm['TEXT'])
sm_v = tokenizer.texts_to_sequences(sm['TEXT'])

sm_v[0]

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(md['TEXT'])
md_v = tokenizer.texts_to_sequences(md['TEXT'])

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(lg['TEXT'])
lg_v = tokenizer.texts_to_sequences(lg['TEXT'])

avg_len_sm = int(avg_len_sm)

avg_len_md = int(avg_len_md)

avg_len_lg = int(avg_len_lg)

for sentence in sm_v:
    while len(sentence) < avg_len_sm: # avg_len_sm 마지막에 구한 걸로 통일돼 있을거야
        sentence.append(0)

padded_sm_avg = np.array(sm_v)
padded_sm_avg[0]

for sentence in md_v:
    while len(sentence) < avg_len_md:
        sentence.append(0)

padded_md_avg = np.array(md_v)
padded_md_avg[0]

for sentence in lg_v:
    while len(sentence) < avg_len_lg:
        sentence.append(0)

padded_lg_avg = np.array(lg_v)
padded_lg_avg[0]

pip install sdmetrics

pip install sdv

import random
import time
from operator import attrgetter
import pandas as pd
from sdv.metrics.tabular import *
import warnings

warnings.filterwarnings("ignore")
pd.set_option('display.max_colwidth', -1)
pd.set_option('display.max_columns', None)
other_metrics_dict = {
    #'BNLikelihood': BNLikelihood,
    #'BNLogLikelihood': BNLogLikelihood,
    #'LogisticDetection': LogisticDetection,
    #'SVCDetection': SVCDetection,
    #'GMLogLikelihood': GMLogLikelihood,
    'CSTest': CSTest, #work this metric first
    #'KSTest': KSComplement, #work this metric first
    #'ContinuousKLDivergence': ContinuousKLDivergence,
    'DiscreteKLDivergence': DiscreteKLDivergence
}

privacy_categorical_metrics_dict = {
    'CategoricalCAP': CategoricalCAP,
    'CategoricalZeroCAP': CategoricalZeroCAP,
    'CategoricalGeneralizedCAP': CategoricalGeneralizedCAP,
    'CategoricalKNN': CategoricalKNN,
    'CategoricalNB': CategoricalNB,
    'CategoricalRF': CategoricalRF,
    # 'CategoricalEnsemble': CategoricalEnsemble
}

privacy_numerical_metrics_dict = {
    'NumericalMLP': NumericalMLP,
    'NumericalLR': NumericalLR,
    'NumericalSVR': NumericalSVR,
    # 'NumericalRadiusNearestNeighbor': NumericalRadiusNearestNeighbor # This metrics is too slow, so we will not use.
}

def create_table(real_data, synthetic_data, numerical_columns, key_fields, sensitive_fields, target_block_size=30): #originally the block size was 100
    """
    The function to create table
    :param real_data: The original data
    :param synthetic_data: The synthetic data generate by tools, should be a List of pandas.DataFrame
    :param numerical_columns: A list of string, telling the function what columns are numerical columns
    :param key_fields: The key field that we need to calculate privacy and utility
    :param sensitive_fields: The sensitive field that we need to calculate privacy and utility. -> outcomes
    :param target_block_size: How many rows in each block, the final result will be average of all blocks
    :return: result_n(c)_avg: the numerical(categorical) result for each metrics,
    numerical(categorical)_avg: the average of all metrics, total_score: the average of numerical and categorical
    """
    #total_len = len(real_data)
    #total_len = total_len // target_block_size * target_block_size # -> why should I divide it by target_block_size?

    real_n, real_c = split_table(real_data, numerical_col=numerical_columns)

    key_fields_n, key_fields_c, sensitive_fields_n, sensitive_fields_c = get_meta_data(real_n,
                                                                                       real_c,
                                                                                       key_fields,
                                                                                       sensitive_fields)

    data_dict_n = {
        'real': real_n
    }
    data_dict_c = {
        'real': real_c
    }

    for i, syn_data in enumerate(synthetic_data):
        syn_n, syn_c = split_table(syn_data, numerical_columns)
        data_dict_n[f"level{i}"] = syn_n
        data_dict_c[f"level{i}"] = syn_c

    data_dict_n = remove_nan_in_numerical(data_dict_n)

    meta_data_n = {
        'key_fields': key_fields_n,
        'sensitive_fields': sensitive_fields_n,
    }
    meta_data_c = {
        'key_fields': key_fields_c,
        'sensitive_fields': sensitive_fields_c,
    }
    metric_list_c = privacy_categorical_metrics_dict.keys()
    metric_list_n = privacy_numerical_metrics_dict.keys()

    result_n = []
    result_c = []

    #for index in range(0, total_len, target_block_size):
    partial_n = get_part_of_data(data_dict_n, target_block_size,index = 0) # 앞에서 data_dict와 같은 역할
    partial_c = get_part_of_data(data_dict_c, target_block_size,index = 0)
    partial_real_n = partial_n['real']
    partial_real_c = partial_c['real']
    partial_n.pop('real')
    partial_c.pop('real')


    numerical = _create_table(metric_list_n, partial_real_n,
                              partial_n, meta_data_n, mode='n')
    print('here')#-> just for debugging
    categorical = _create_table(metric_list_c, partial_real_c, partial_c, meta_data_c, mode='c')
    print('here')#-> just for debugging
    result_n.append(numerical)
    result_c.append(categorical) # 왜 append 되는게 없을까?



    # starting for privacy
    # Assuming you have already defined other_metrics_dict, synthetic_data, and meta_data
    # Compute other metrics
    computed_metrics = compute_other_metrics(other_metrics_dict, partial_real_c, partial_c)

    # Print or use the computed metrics
    for metric_name, metric_value in computed_metrics.items():
        print(f"{metric_name}: {metric_value}")
    #end for privacy



    result_n_avg = sum(result_n) / len(result_n)
    result_c_avg = sum(result_c) / len(result_c)
    numerical_avg = result_n_avg.mean()
    numerical_avg['time'] *= len(synthetic_data)
    categorical_avg = result_c_avg.mean()
    categorical_avg['time'] *= len(synthetic_data)

    total_score = (numerical_avg + categorical_avg) / 2

    return result_n_avg, result_c_avg, numerical_avg, categorical_avg, total_score

def compute_other_metrics(metrics_dict, _real_data, synthetic_data): # make another create_table just for privacy
    global other_metrics_dict
    # all_df = {}
    # for meta_data in meta_data_list:
    result = {}
    times = {}
    success_metrics = []
    for level, data in synthetic_data.items(): # change the name for the level
        scores = []
        for metrics in metrics_dict:
            print(metrics) # just for debugging
            _start = time.time()
            if metrics not in metrics_dict.keys():
                continue
            metric = metrics_dict[metrics]
            score = metric.compute(real_data=_real_data, synthetic_data=data)
            print('score :',score) # just for debugging
            scores.append(score)
            if metrics not in success_metrics:
                success_metrics.append(metrics)
                times[metrics] = 0
            _end = time.time() - _start
            times[metrics] += _end
        result[level] = scores
    result['time'] = [times[key] for key in success_metrics]
    df = pd.DataFrame(result, index=success_metrics)
    # df['time'] = times
    # all_df[meta_data['sensitive_fields'][0]] = df

    print('one table generated')
    return df

def get_meta_data(real_data_n, real_data_c, key_fields, sensitive_fields):
    key_fields_c = [key for key in key_fields if key in real_data_c.columns]
    key_fields_n = [key for key in key_fields if key in real_data_n.columns]
    sensitive_fields_c = [key for key in sensitive_fields if key in real_data_c.columns]
    sensitive_fields_n = [key for key in sensitive_fields if key in real_data_n.columns]
    return key_fields_n, key_fields_c, sensitive_fields_n, sensitive_fields_c

def _create_table(metrics_list, _real_data, synthetic_data_dict: dict, meta_data, mode): #generate the privacy metrics (The colon in synthetic_data_dict: dict is a type hint in Python. It is used to specify the expected type of the synthetic_data_dict parameter.)
    global privacy_numerical_metrics_dict, privacy_categorical_metrics_dict
    # all_df = {}
    # for meta_data in meta_data_list:
    result = {}
    times = {}
    metrics_dict = privacy_numerical_metrics_dict if mode == 'n' else privacy_categorical_metrics_dict
    success_metrics = []
    for level, data in synthetic_data_dict.items(): # change the name for the level
        scores = []
        for metrics in metrics_list:
            print(metrics) # just for debugging
            _start = time.time()
            if metrics not in metrics_dict.keys():
                continue
            metric = metrics_dict[metrics]
            score = metric.compute(real_data=_real_data, synthetic_data=data, **meta_data)
            print('score :',score) # just for debugging
            scores.append(score)
            if metrics not in success_metrics:
                success_metrics.append(metrics)
                times[metrics] = 0
            _end = time.time() - _start
            times[metrics] += _end
        result[level] = scores
    result['time'] = [times[key] for key in success_metrics]
    df = pd.DataFrame(result, index=success_metrics)
    # df['time'] = times
    # all_df[meta_data['sensitive_fields'][0]] = df

    print('one table generated')
    return df


def split_table(data_table: pd.DataFrame, numerical_col=None): # for spliting the num and cat
    if numerical_col:
        numerical_data = data_table[numerical_col]
        categorical_data = data_table.drop(numerical_col, axis=1).applymap(str)
        return numerical_data, categorical_data

    dtype_list = data_table.dtypes.apply(attrgetter('kind'))
    numerical_index = []
    categorical_index = []
    for i, dtype in enumerate(dtype_list):
        if dtype == 'f' or dtype == 'i':
            numerical_index.append(i)
        else:
            categorical_index.append(i)
    return data_table.iloc[:, numerical_index], data_table.iloc[:, categorical_index]


def get_metadata(table: pd.DataFrame):
    meta_data_list = []
    clm = list(table.columns)
    # for i, column in enumerate(clm):
    #     m_d = {
    #         'key_fields': clm[:i] + clm[i + 1:],
    #         'sensitive_fields': [column]
    #     }
    #     meta_data_list.append(m_d)
    index = random.randint(0, len(clm) - 1)
    index = [1, 2]
    meta_data_list.append({
        'key_fields': [clm[i] for i in range(len(clm)) if i not in index],
        'sensitive_fields': [clm[i] for i in index]
    })
    return meta_data_list


def remove_nan_in_numerical(numerical_table_dict: dict):
    for k, v in numerical_table_dict.items():
        numerical_table_dict[k] = v.fillna(0)
    return numerical_table_dict


def get_part_of_data(table_dict, n, index): # as a result of regarding the for loop above, I fixed the index as 1. 인덱스는 시작점 역할을 하는 것 같음
    new_dict = {}
    # index = random.randint(0, 14691 - n)
    # index = 1
    for k, v in table_dict.items():
        new_dict[k] = v.iloc[index:index + n, :] #index is the start of the row and index + n is the end of the row
        new_dict[k].reset_index(drop=True, inplace=True)
    return new_dict

col_list=[]
org = pd.read_csv('stay_length_4000_with_notes (2).csv')
(org.dtypes)
columns = list(org.columns)
for col in columns:
  print(org[col].dtypes)
  if org[col].dtypes != 'object':
    col_list.append(col)
del col_list[0]
col_list

col_list

from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()

tokenizer.fit_on_texts(org)
text_sequences = tokenizer.texts_to_sequences(org)

word_vocab = tokenizer.word_index

word_vocab

ob_col = []
for col in columns:
    if org[col].dtypes == 'object':
        ob_col.append(col)

ob_col

for col in ob_col:
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(col[0])
    first_obj = tokenizer.texts_to_sequences(col[0])
    if len(first_obj) > 5 :
        print(col)

for col in columns:
    print(col,org[col].unique())

org

copy = org.copy()

copy = copy.drop(columns=['TEXT'])

copy

sm_fin = pd.concat([copy, sm], axis=1)

sm_fin = sm_fin.drop(columns=['Unnamed: 0'])

sm_fin

md_fin = pd.concat([copy, md], axis=1)

md_fin = md_fin.drop(columns=['Unnamed: 0'])

md_fin

lg_fin = pd.concat([copy, lg], axis=1)

lg_fin = lg_fin.drop(columns=['Unnamed: 0'])

lg_fin

if __name__ == '__main__':
    numerical_column = col_list
    _real_data = org
    synthetic_data_small = sm_fin
    synthetic_data_medium = md_fin
    synthetic_data_large = lg_fin

    data_dict = {
        'real': _real_data,
        'small': synthetic_data_small,
        'medium': synthetic_data_medium,
        'large': synthetic_data_large,
    }

    data_dict = get_part_of_data(data_dict, len(data_dict['real']), 0) # -> second hyper para is target_block_size and it decides the size of the row.

    _real_data = data_dict['real']
    data_dict.pop('real')
    data = data_dict.values()
    _key_fields = ['length_of_stay_sum','plength_of_stay_avg','Race','Gender'] # -> should include one for each cat and num, otherwise it will show an error saying it's empty
    _sensitive_fields = ['Heart.Rate','HCO3..serum.','Anion.gap','Potassium..serum.']
    rn, rc, an, ac, t = create_table(_real_data, data, numerical_column, _key_fields, _sensitive_fields)
    print('result_n = ',rn)
    print('\n')
    print('result_c = ',rc)
    print('\n')
    print('avg_n = ',an)
    print('\n')
    print('avg_c = ',ac)
    print('\n')
    print('time = ',t)
    # real_data_n, real_data_c = split_table(_real_data, numerical_col=numerical_column)
    # synthetic_data_small_n, synthetic_data_small_c = split_table(synthetic_data_small, numerical_col=numerical_column)
    # synthetic_data_medium_n, synthetic_data_medium_c = split_table(synthetic_data_medium,
    #                                                                numerical_col=numerical_column)
    # synthetic_data_large_n, synthetic_data_large_c = split_table(synthetic_data_large, numerical_col=numerical_column)

    # drop the error column
    # synthetic_data_small_n = synthetic_data_small_n.drop(columns=['bill_prov_zip_cd', 'rndrng_prov_zip_cd'])
    # synthetic_data_medium_n = synthetic_data_medium_n.drop(columns=['bill_prov_zip_cd', 'rndrng_prov_zip_cd'])
    # synthetic_data_large_n = synthetic_data_large_n.drop(columns=['rndrng_prov_zip_cd'])
    # real_data_c = real_data_c.drop(columns=['bill_prov_zip_cd', 'rndrng_prov_zip_cd'])
    # synthetic_data_large_c = synthetic_data_large_c.drop(columns=['bill_prov_zip_cd'])

    # group the tables
    # synthetic_data_c = {
    #     'real': real_data_c,
    #     'small': synthetic_data_small_c,
    #     'medium': synthetic_data_medium_c,
    #     'large': synthetic_data_large_c
    # }
    # synthetic_data_n = {
    #     'real': real_data_n,
    #     'small': synthetic_data_small_n,
    #     'medium': synthetic_data_medium_n,
    #     'large': synthetic_data_large_n
    # }
    # synthetic_data_n = remove_nan_in_numerical(synthetic_data_n)
    #
    # mc_list_c = privacy_categorical_metrics_dict.keys()
    # mc_list_n = privacy_numerical_metrics_dict.keys()
    # meta_data_list_n = get_metadata(real_data_n)
    # meta_data_list_c = get_metadata(real_data_c)
    # total_length_n = len(real_data_n)
    # total_length_c = len(real_data_c)
    # num = 100
    # partial_data_n = get_part_of_data(synthetic_data_n, num, 0)
    # partial_data_c = get_part_of_data(synthetic_data_c, num, 0)
    # start = time.time()
    #
    # preal_data_n = partial_data_n['real']
    # partial_data_n.pop('real')
    # numerical_1 = _create_table(mc_list_n, preal_data_n,
    #                             partial_data_n, meta_data_list_n[0], mode='n')
    # preal_data_c = partial_data_c['real']
    # partial_data_c.pop('real')
    # categorical_1 = _create_table(mc_list_c, preal_data_c,
    #                               partial_data_c, meta_data_list_c[0], mode='c')
    #
    # partial_data_n = get_part_of_data(synthetic_data_n, num, 200)
    # partial_data_c = get_part_of_data(synthetic_data_c, num, 200)
    #
    # preal_data_n = partial_data_n['real']
    # partial_data_n.pop('real')
    # numerical_2 = _create_table(mc_list_n, preal_data_n,
    #                             partial_data_n, meta_data_list_n[0], mode='n')
    # preal_data_c = partial_data_c['real']
    # partial_data_c.pop('real')
    # categorical_2 = _create_table(mc_list_c, preal_data_c,
    #                               partial_data_c, meta_data_list_c[0], mode='c')

