



def text_clean(text, control):
    text = text.copy()

    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))
    for i in range(len(text)):
        text[i] = extract_word(text[i])
        if control['lemmatize']:
            for j in range(len(text[i])):
                text[i][j] = lemmatizer.lemmatize(text[i][j])

    if control['stop_words']:
        for i in range(len(text)):
            for j in range(len(text[i])):
                if text[i][j] in stop_words:
                    text[i][j] = ""

    if control['remove_number']:
        for i in range(len(text)):
            for j in range(len(text[i])):
                if text[i][j].isnumeric():
                    text[i][j] = ""

    for i in range(len(text)):
        text[i] = ' '.join(text[i])

    vectorizer = CountVectorizer(max_df=0.8, min_df=3, ngram_range=(1,control['gram']))
    X = vectorizer.fit_transform(text)
    return vectorizer.get_feature_names(), X.toarray()

def generate_model(X, y, num_class):
    x_train_all, x_predict, y_train_all, y_predict = train_test_split(X, y, test_size=0.10, random_state=100)
    x_train, x_test, y_train, y_test = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=100)
    train_data = lgb.Dataset(data=x_train, label=y_train)
    test_data = lgb.Dataset(data=x_test, label=y_test)
    param = {'num_leaves': 31, 'objective': 'multiclass', 'num_class':num_class}
    param['metric'] = 'multi_logloss'
    num_round = 10
    bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])
    # dd = bst.trees_to_dataframe()
    # print(dd)
    # graph = lgb.create_tree_digraph(bst)
    # graph.render()
    # return ["awd", "adw"]
    return bst

def get_features(aims, df):
    titles = df.columns.values.tolist()
    nlp = spacy.load("en_core_web_lg")
    new_aims = aims.copy()
    curr_feature = titles[0]
    for i in range(len(aims)):
        aim = word_pp(aims[i])
        doc1 = nlp(aim)
        best_sim = 0
        for title in titles:
            temp_t = word_pp(title)
            doc2 = nlp(temp_t)
            curr_sim = doc1.similarity(doc2)
            if curr_sim > best_sim:
                best_sim = curr_sim
                curr_feature = title
        new_aims[i] = curr_feature
    return new_aims


def word_pp(word):
    word = list(word)
    for i in range(len(word)):
        if word[i] in string.punctuation:
            word[i] = ' '
    return ''.join(word)

def extract_word(input_string):
    pu = string.punctuation
    for p in pu:
        input_string = input_string.replace(p, ' ')
    return input_string.lower().split()


# TODO: percentage way
def split_class(y, n):
    m = np.mean(y)
    y[y > m * 2] = m * 2
    sd = max(y) / n
    for i in range(len(y)):
        for j in range(n):
            if y[i] >= j * sd and y[i] <= (j + 1)*sd:
                y[i] = j







def text_clean(text, control):
    text = text.copy()

    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))
    for i in range(len(text)):
        text[i] = extract_word(text[i])
        if control['lemmatize']:
            for j in range(len(text[i])):
                text[i][j] = lemmatizer.lemmatize(text[i][j])

    if control['stop_words']:
        for i in range(len(text)):
            for j in range(len(text[i])):
                if text[i][j] in stop_words:
                    text[i][j] = ""

    if control['remove_number']:
        for i in range(len(text)):
            for j in range(len(text[i])):
                if text[i][j].isnumeric():
                    text[i][j] = ""

    for i in range(len(text)):
        text[i] = ' '.join(text[i])

    vectorizer = CountVectorizer(max_df=0.8, min_df=3, ngram_range=(1,control['gram']))
    X = vectorizer.fit_transform(text)
    return vectorizer.get_feature_names(), X.toarray()

def generate_model(X, y, num_class):
    x_train_all, x_predict, y_train_all, y_predict = train_test_split(X, y, test_size=0.10, random_state=100)
    x_train, x_test, y_train, y_test = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=100)
    train_data = lgb.Dataset(data=x_train, label=y_train)
    test_data = lgb.Dataset(data=x_test, label=y_test)
    param = {'num_leaves': 31, 'objective': 'multiclass', 'num_class':num_class}
    param['metric'] = 'multi_logloss'
    num_round = 10
    bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])
    # dd = bst.trees_to_dataframe()
    # print(dd)
    # graph = lgb.create_tree_digraph(bst)
    # graph.render()
    # return ["awd", "adw"]
    return bst

def get_features(aims, df):
    titles = df.columns.values.tolist()
    nlp = spacy.load("en_core_web_lg")
    new_aims = aims.copy()
    curr_feature = titles[0]
    for i in range(len(aims)):
        aim = word_pp(aims[i])
        doc1 = nlp(aim)
        best_sim = 0
        for title in titles:
            temp_t = word_pp(title)
            doc2 = nlp(temp_t)
            curr_sim = doc1.similarity(doc2)
            if curr_sim > best_sim:
                best_sim = curr_sim
                curr_feature = title
        new_aims[i] = curr_feature
    return new_aims


def word_pp(word):
    word = list(word)
    for i in range(len(word)):
        if word[i] in string.punctuation:
            word[i] = ' '
    return ''.join(word)

def extract_word(input_string):
    pu = string.punctuation
    for p in pu:
        input_string = input_string.replace(p, ' ')
    return input_string.lower().split()


# TODO: percentage way
def split_class(y, n):
    m = np.mean(y)
    y[y > m * 2] = m * 2
    sd = max(y) / n
    for i in range(len(y)):
        for j in range(n):
            if y[i] >= j * sd and y[i] <= (j + 1)*sd:
                y[i] = j



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns



#pip install contractions


# Load the Drive helper and mount
#from google.colab import drive
#import os
#drive.mount('/content/drive')


#os.chdir('/content/drive/My Drive/Colab Notebooks')


%pip install nltk


import nltk


nltk.download('stopwords')


from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
stopwords = stopwords.words('english')
import re
#!pip install sdv gensim unidecode contractions
import contractions
from sklearn.feature_extraction.text import CountVectorizer


import nltk
nltk.download('punkt')# for the tokenize test
nltk.download('stopwords')# stopwords dict
nltk.download('wordnet')
nltk.download('omw-1.4') #Open Multilingual WordNet version 1.4


#path = "/content/drive/My Drive/mimic3.csv"
raw = pd.read_csv('data/mimi3.csv')['TEXT']
raw_df = raw.copy() #just dealing with the TEXT column


lemmatizer = WordNetLemmatizer() #word lemmatizer
proterstemmer = PorterStemmer() # word stemmer
raw_df.info()





raw_df.isnull().sum()


raw_df.head()


# replace the non-character with " "
raw_df = raw_df.apply(lambda x:
             " ".join(re.sub(r'[^a-zA-Z]', " ", w).lower() for w in x.split()
             if re.sub(r'^a-zA-Z', ' ', w).lower() not in stopwords))


raw[0]


raw_df[0]


# deal with contraction
def decontracted(phrase):
    """decontracted takes text and convert contractions into natural form.
     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490"""

    # specific
    phrase = re.sub(r"won\'t", "will not", phrase)
    phrase = re.sub(r"can\'t", "can not", phrase)

    # general
    phrase = re.sub(r"n\'t", " not", phrase)
    phrase = re.sub(r"\'re", " are", phrase)
    phrase = re.sub(r"\'s", " is", phrase)
    phrase = re.sub(r"\'d", " would", phrase)
    phrase = re.sub(r"\'ll", " will", phrase)
    phrase = re.sub(r"\'t", " not", phrase)
    phrase = re.sub(r"\'ve", " have", phrase)
    phrase = re.sub(r"\'m", " am", phrase)
    return phrase

def expand_contractions(phrase):
    """use contractions package to do decontraction"""
    phrase = contractions.fix(phrase)
    return phrase


# print(expand_contractions('won\'t')) -> will not
# print(decontracted('won\'t')) -> will not






def preprocessing(notes):
    """
    preprocess notes including lemmatization, decontraction, stemming, lowercase,
    return notes(preprocessed notes), tokenized_notes(list)
    """
    tokenized_notes = []
    for i, text in enumerate(notes):
        notes[i] = decontracted(text) # here we use the decontracted function
        text_word_list = word_tokenize(text)

        # expand contraction
        text_word_list = [expand_contractions(word) for word in text_word_list]
        # lemmatization
        text_word_list = [lemmatizer.lemmatize(word) for word in text_word_list]
        # stemming
        text_word_list = [proterstemmer.stem(word) for word in text_word_list]
        # to lower
        text_word_list = [word.lower() for word in text_word_list]

        tokenized_notes.append(text_word_list)
        notes[i] = " ".join(text_word_list)
    return notes, tokenized_notes



notes_list, tokenized_notes = preprocessing(raw_df)


print(len(notes_list), "notes")


notes_list[0]





outcomes = ['length_of_stay_avg', 'Religion', 'Gender'] # selected these three predictors





import sys
sys.path.insert(0,'/Users/joseph/Desktop/Datasifter') #to use the function that we made


%pip install lightgbm
import lightgbm as lgb
from matplotlib import pyplot as plt
%pip install spacy



!python -m spacy download en_core_web_lg # had a problem with jupyter, but I used virtual env. so if you are not using vitual env, you may try in jupyter


consider_text = True # consider text OR features
num_keywords = 10 # num keywords generated
drop_list = [0, 2, 4, 5, 6] # useless df features
num_class = 5 # for continuous outcomes
control = {'remove_number': True, 'lemmatize': True, 'normalize': True, 'stop_words': True, 'gram': 1, 'remove_name': True}
# read data frame
df = pd.read_csv('data/mimi3.csv')
df = df.drop(df.columns[drop_list], axis=1)
# get all categorical columns
cat_columns = df.select_dtypes(['object']).columns
# convert all categorical columns to numeric
df[cat_columns] = df[cat_columns].apply(lambda x: pd.factorize(x)[0])
df.info()


# change text to preprocessed text
df['TEXT'] = notes_list


# detect outcome column

#import functions
import spacy
outcomes = get_features(outcomes, df)


#!conda install -c conda-forge spacy-model-en_core_web_lg--y
import sys
print(sys.executable)


import en_core_web_lg
nlp = en_core_web_lg.load()


#pip install en_core_web_lg


#!python -m spacy download en_core_web_lg
#!/Users/suhyunjung/.pyenv/versions/3.9.16/lib/python3.9 -m spacy download en_core_web_lg


import spacy
from spacy.lang.en.examples import sentences


nlp = spacy.load("en_core_web_lg")








import lightgbm as lgb
from sklearn.model_selection import train_test_split


np.array(df[outcomes[0]])


keywords = {} # to get a feture importance result (final result from LightGBM)
for outcome in outcomes:
    y = np.array(df[outcome])
    num_class = len(np.unique(y))
    X = np.array(df['TEXT'])
    #vectorizer = CountVectorizer(max_df=0.8, min_df=3, ngram_range=(1,control['gram']))
    vectorizer = CountVectorizer(min_df=0.2, ngram_range=(1,1)) # unigrams
    X = vectorizer.fit_transform(notes_list).toarray() #This converts the text data into a matrix that machine learning algorithms can work with.
    name= vectorizer.get_feature_names_out() # if your sklear version is diff use 'vectorizer.get_feature_names()


    x_train_all, x_predict, y_train_all, y_predict = train_test_split(X, y, test_size=0.15, random_state=100)
    x_train, x_test, y_train, y_test = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=100)
    train_data = lgb.Dataset(data=x_train, label=y_train)
    test_data = lgb.Dataset(data=x_test, label=y_test)
    param = {'num_leaves': 31, 'objective': 'multiclass', 'num_class':num_class}
    param['metric'] = 'multi_logloss'
    num_round = 10
    evals = {}
    model = lgb.train(param, train_data, num_round, valid_sets=[test_data], callbacks= [lgb.record_evaluation(evals)])
    # dd = bst.trees_to_dataframe()
    #     if consider_text:
    #         name, X = text_clean(np.array(df['TEXT']), control)
    #         print(X.shape)
    #         break
    #         if control['normalize']:
    #             X = (X - X.mean(axis=0)) / X.std(axis=0)
    #     else:
    #         temp_df = df.drop(['TEXT'], axis=1)
    #         X = np.array(temp_df.drop(outcome, axis=1))
    #         name= np.array(temp_df.columns)

    # model = generate_model(X, y, num_class)
    feature_imp = pd.DataFrame({"value": model.feature_importance(), 'Feature': vectorizer.get_feature_names_out()})
    keywords[outcome] = pd.DataFrame(feature_imp.sort_values(by=feature_imp.columns[0], ascending=False)[0:num_keywords])
    lgb.plot_metric(evals)



import pickle
with open('keywords.pkl', 'wb') as f:
    pickle.dump(keywords, f)


#with open('keywords.pkl', 'rb') as f:
#    loaded_dict = pickle.load(f)


outcomes


# words significantly predicts sensitive factor: Religion
# before preprocessing
keywords[outcomes[2]]


# after preprocessing
keywords[outcomes[2]]





import multiprocessing
cores = multiprocessing.cpu_count()


from gensim.models import Word2Vec
import random

from pyparsing import WordStart

class MySentences(object): #It splits each sentence into words and yields them as separate tokens. The class also implements methods for length retrieval and item retrieval, which allows treating an instance of this class as a list-like object.
    def __init__(self, text):
        self.text = text

    def __iter__(self):
        for line in self.text:
            yield line.split()

    def __len__(self):
        return len(self.text)

    def __getitem__(self, i):
        return self.text[i]

    def __setitem__(self, index, item1):
        self.text[index] = item1

class W2V(object):
    def __init__(self, sentences, cores, vec_size) -> None:
        self.sentences = sentences
        self.model = Word2Vec(min_count=10,
                     window=10,
                     vector_size=vec_size,
                     sample=6e-5,
                     alpha=0.03,
                     min_alpha=0.0007,
                     negative=20,
                     workers=cores-1)

    def train(self):
        self.model.build_vocab(self.sentences)
        self.model.train(self.sentences, total_examples=self.model.corpus_count, epochs=30, report_delay=1)

    def get_similar_word(self, keywords=["wife"], start_radius=0, end_radius=100): # Returns a random similar word for each keyword within the specified radius range. It queries the model for the most similar words to each keyword.
        word_list = [self.model.wv.most_similar(keywords[i], topn=end_radius)[start_radius:] for i in range(len(keywords))]
        return [random.choice(words)[0] for words in word_list]

    def get_similar_word_2(self, keywords=["wife"], start_radius=0, end_radius=100): # it returns a list of lists of similar words for each keyword. It doesn't choose a random word from each list.
        #print(len(keywords))
        #print(self.model.wv.most_similar(keywords[i], topn=end_radius))
        word_list = [self.model.wv.most_similar(keywords[i], topn=end_radius)[start_radius:] for i in range(len(keywords))]
        return word_list


    def get_similar_word_list(self, keywords, start_radius = 0, end_radius = 100):
        if isinstance(keywords, list):
            TypeError("keywords should be a list")
        else:
            keywords = [keywords]
            for keyword in keywords:
                word_list = self.model.wv.most_similar(keyword, topn = end_radius)
        return word_list


    def save(self, route): #Saves the trained Word2Vec model
        self.model.save(route)

    def load(self, route): #loads the trained Word2Vec model
        self.model = Word2Vec.load(route)

    def find_word_idx(self, keywords=["wife"]): #Finds the indices of sentences containing each keyword in the provided list of keywords.
        return [[i for i in range(len(self.sentences)) if keywords[j] in self.sentences[i]] for j in range(len(keywords))]

    def obfuscate(self, sentence, keywords=["wife"]): #Replaces keywords in a sentence with similar words obtained from the model.
        word_list = self.get_similar_word(keywords=keywords)
        for i in range(len(keywords)):
            sentence = sentence.replace(keywords[i], word_list[i])
        return sentence





sentences = MySentences(df.TEXT)
sentences


w2v_model = W2V(sentences, 1, 300) # sentences , core, vec_size


w2v_model.train()


w2v_model.save("w2vmodel")


#from google.colab import files
#uploaded = files.upload()


import pickle
with open('keywords.pkl', 'rb') as f:
    keywords = pickle.load(f)





print(outcomes)


#store the first 10 most predictive words for each sensitive outcomes

# length_of_stay_avg
LOSA_keywords = keywords[outcomes[0]]
# Gender
Gender_keywords = keywords[outcomes[1]]
# Religion
Religion_keywords = keywords[outcomes[2]]

LOSA_list = list(LOSA_keywords.Feature)
Gender_list = list(Gender_keywords.Feature)
Religion_list = list(Religion_keywords.Feature)
# LOS_list = list(LOS_keywords.Feature)
# LOS_list.pop(3)

# Gender_list = list(Gender_keywords.Feature)
# Gender_list.pop(-2)

# Religion_list = list(Religion_keywords.Feature)
# Religion_list.pop(6)
# Religion_list.pop()


keywords_dict = {}
keywords_dict.setdefault(outcomes[0], LOSA_list)
keywords_dict.setdefault(outcomes[1], Gender_list)
keywords_dict.setdefault(outcomes[2], Religion_list)



def find_3levels_word(keyword_list, small = 33, mid = 66):
    """"keywords is a list of predictive words of a certain outcome,
    small is the percentile criteria for small level of obfuscation,
    mid is the percentile criteria for mid level of obfuscation,

    return are 3 word list: each derived by random sampling from the corresponding obfuscation level"""

    small_level_list = []
    mid_level_list = []
    large_level_list = []
    for word in keyword_list:
        try:
            similar_words = sorted(w2v_model.get_similar_word_list(keywords = word, end_radius = 100), key = lambda x: x[1], reverse = True)
        except:
            continue
        else:
            word_list, cos_list = list(zip(*similar_words)) #instead of spliting by the number use cdf // make it visible with some metrics //change the number of keywords 10 -> bigger
            small_range = word_list[:small]# begin ~ small
            mid_range = word_list[small:mid] #small ~ mid
            large_range = word_list[mid:] # mid ~ end
            small_level_list.append(random.sample(small_range,1)[0])
            mid_level_list.append(random.sample(mid_range,1)[0])
            large_level_list.append(random.sample(large_range,1)[0])

    return small_level_list, mid_level_list, large_level_list





def get_replace_word(keywords, level, small = 33, mid = 66):
    if isinstance(level, str):
        small, mid, large = find_3levels_word(keywords, small, mid)
        level = level.lower()
        if level == "small":
            return small
        elif level == "mid":
            return mid
        elif level == "large":
            return large
    else: NotImplemented


keywords_dict['Gender'] #this is original 10 keywords from Gender that we are going to change


get_replace_word(keywords_dict['Gender'], 'small') # obfuscation with small_list words for Gender 10keywords


outcomes


print(len(get_replace_word(keywords_dict['Gender'], "small")), "\n",
      get_replace_word(keywords_dict['Gender'], "small"), "\n\n",
     len(get_replace_word(keywords_dict['Gender'], "mid")), "\n",
      get_replace_word(keywords_dict['Gender'], "mid"), "\n\n",
     len(get_replace_word(keywords_dict['Gender'], "large")), "\n",
      get_replace_word(keywords_dict['Gender'], "large"),)


pd.DataFrame({'keywords':keywords_dict['Religion'],
             'small_level':get_replace_word(keywords_dict['Religion'], "small"),
             'mid_level':get_replace_word(keywords_dict['Religion'], "mid"),
             'large_level':get_replace_word(keywords_dict['Religion'], "large")})


pd.DataFrame({'keywords':keywords_dict['Gender'],
             'small_level':get_replace_word(keywords_dict['Gender'], "small"),
             'mid_level':get_replace_word(keywords_dict['Gender'], "mid"),
             'large_level':get_replace_word(keywords_dict['Gender'], "large")})


print(len(keywords_dict['length_of_stay_avg']))
print(len(get_replace_word(keywords_dict['length_of_stay_avg'], "small")),
len(get_replace_word(keywords_dict['length_of_stay_avg'], "mid")),
len(get_replace_word(keywords_dict['length_of_stay_avg'], "large")))
print(keywords_dict['length_of_stay_avg'])


# # ! pip install wikipedia2vec
# from wikipedia2vec import Wikipedia2Vec
# wiki2vec = Wikipedia2Vec.load("enwiki_20180420_100d.pkl")
# wiki2vec.most_similar(wiki2vec.get_word("tarch"), 100)[1:]
# keywords_dict['length_of_stay_avg']
# sorted([(i[0].text, i[1]) for i in wiki2vec.most_similar(wiki2vec.get_word("tarch"), 101)[1:]], key = lambda x:x[1], reverse = True)
# # wiki2vec.most_similar(wiki2vec.get_word('yoda'), 5)
# def find_3levels_word_pretrained(keyword_list, small = 33, mid = 66):
#     """"keywords is a list of predictive words of a certain outcome,
#     small is the percentile criteria for small level of obfuscation,
#     mid is the percentile criteria for mid level of obfuscation,

#     return are 3 word list: each derived by random sampling from the corresponding obfuscation level"""

#     small_level_list = []
#     mid_level_list = []
#     large_level_list = []
#     for word in keyword_list:
# #         try:
# #             similar_words = sorted([(i[0].text, i[1]) for i in wiki2vec.most_similar(wiki2vec.get_word(word), 100)[1:101]], key = lambda x:x[1], reverse = True)
# #             similar_words = sorted(wiki2vec.get_similar_word_list(keywords = word, end_radius = 100), key = lambda x: x[1], reverse = True)
# #         except:

# #         else:
#         similar_words = sorted([(i[0].text, i[1]) for i in wiki2vec.most_similar(wiki2vec.get_word(word), 100)[1:101]], key = lambda x:x[1], reverse = True)
#         print(word)
#         word_list, cos_list = list(zip(*similar_words))
#         small_range = word_list[:small]
#         mid_range = word_list[small:mid]
#         large_range = word_list[mid:]

#         small_level_list.append(random.sample(small_range,1)[0])
#         mid_level_list.append(random.sample(mid_range,1)[0])
#         large_level_list.append(random.sample(large_range,1)[0])
#         print(small_level_list)

#     return small_level_list, mid_level_list, large_level_list

# def get_replace_word(keywords, level, small = 33, mid = 66):
#     if isinstance(level, str):
#         small, mid, large = find_3levels_word_pretrained(keywords, small, mid)
#         level = level.lower()
#         if level == "small":
#             return small
#         elif level == "mid":
#             return mid
#         elif level == "large":
#             return large
#     else: NotImplemented

# # find_3levels_word_pretrained(keywords_dict['length_of_stay_avg'])
# get_replace_word(keywords_dict['length_of_stay_avg'], "small")



# use google news trained w2v
import gensim
model = gensim.models.keyedvectors.load_word2vec_format('/content/gdrive/My Drive/Colab Notebooks/SOCR/GoogleNews-vectors-negative300.bin', binary=True)


sorted(model.similar_by_word(word = keywords_dict['length_of_stay_avg'][0], topn = 10),
                                   key = lambda x: x[1], reverse = True)


# wiki2vec.most_similar(wiki2vec.get_word("tarch"), 100)[1:]
# keywords_dict['length_of_stay_avg']

def find_3levels_word_GN(keyword_list, small = 33, mid = 66):
    """"keywords is a list of predictive words of a certain outcome,
    small is the percentile criteria for small level of obfuscation,
    mid is the percentile criteria for mid level of obfuscation,

    return are 3 word list: each derived by random sampling from the corresponding obfuscation level"""

    small_level_list = []
    mid_level_list = []
    large_level_list = []
    for word in keyword_list:
        try:
            similar_words = sorted(model.similar_by_word(word = keywords_dict['length_of_stay_avg'][0], topn = 100),
                                   key = lambda x: x[1], reverse = True)
        except:
            continue
        else:
            word_list, cos_list = list(zip(*similar_words))
            small_range = word_list[:small]
            mid_range = word_list[small:mid]
            large_range = word_list[mid:]
            small_level_list.append(random.sample(small_range,1)[0])
            mid_level_list.append(random.sample(mid_range,1)[0])
            large_level_list.append(random.sample(large_range,1)[0])

    return small_level_list, mid_level_list, large_level_list


def get_replace_word_GN(keywords, level, small = 33, mid = 66):
    if isinstance(level, str):
        small, mid, large = find_3levels_word_GN(keywords, small, mid)
        level = level.lower()
        if level == "small":
            return small
        elif level == "mid":
            return mid
        elif level == "large":
            return large
    else: NotImplemented


keywords_dict[outcomes[0]]


def generate_words_for_replacement(keywords_dictionary):
    """input:
    keywords_dict: dictionary, with key = sensitive factor, value = predictive words of that factor
    return the data frame containing the words for replacement of each sensitive factor"""
    words_for_replacement = {}
    for key, item in keywords_dictionary.items():
        words_for_replacement[key] = pd.DataFrame({'keywords':keywords_dictionary[key],
             'small_level':get_replace_word_GN(keywords_dictionary[key], "small"),
             'mid_level':get_replace_word_GN(keywords_dictionary[key], "mid"),
             'large_level':get_replace_word_GN(keywords_dictionary[key], "large")})



    return words_for_replacement


a = generate_words_for_replacement(keywords_dict)


a[outcomes[2]]





wordsReplacement = generate_words_for_replacement(keywords_dict)


wordsReplacement[outcomes[0]]


wordsReplacement[outcomes[0]]


def obfuscate(notes, wordForReplacement, level, sensitive_factor):
    """wordForReplacement: dictionary containing all senstitive factors and their corresponding words for replacement
    level: obfuscation level (small, mid, large)
    sensitive_factor: obfuscation factor (outcomes)"""
    wordForReplacement = wordsReplacement.copy()
#     sensitive_factor = "Religion"
    all_note = notes.copy()
    for i, note in enumerate(all_note):
        replace_dict = {}
        for word in note.split():
#             print(word)
#             print(list(wordForReplacement[sensitive_factor]['keywords']))
            curr_word = lemmatizer.lemmatize(word).lower()
            if curr_word in list(wordForReplacement[sensitive_factor]['keywords']):
                if level == "small":
#                     print("curr_word:", curr_word)
                    potential_words = wordForReplacement[sensitive_factor]
                    word_for_replace = potential_words[potential_words['keywords'] == curr_word]['small_level'].values[0]
#                     print("word_for_replace", word_for_replace)
                    replace_dict[curr_word] = word_for_replace
#       all_note[i] = re.sub(curr_word, word_for_replace, note)
        print(replace_dict)
        for target, replace in replace_dict.items():
            note = note.replace(target, replace)
#             re.sub(target, replace, note)
        all_note[i] = note

    return all_note






# original
raw[0]


obfuscate([raw[0]], wordsReplacement, "small", outcomes[1])[0]


outcomes


print(raw[0])


print(obfuscate([raw[0]], wordsReplacement, "small", outcomes[1])[0])





# obfuscated
#all_note[0]





# def stat_summary(outcome_keywords_similar_list):
#     #Religion_stat_summary
#     Religion_stat_summary = pd.DataFrame()
#     keywords_similarity = w2v_model.get_similar_word_2(keywords = outcome_keywords_similar_list)
#     max_value, min_value, mean, median, IQ25, IQ75 = [], [], [], [], [], []
#     for word_list in keywords_similarity:
#         max_value.append(np.max(list(zip(*word_list))[1]))
#         min_value.append(np.min(list(zip(*word_list))[1]))
#         mean.append(np.mean(list(zip(*word_list))[1]))
#         median.append(np.median(list(zip(*word_list))[1]))
#         IQ25.append(np.percentile(list(zip(*word_list))[1],25))
#         IQ75.append( np.percentile(list(zip(*word_list))[1],75))

#     stat_summary = pd.DataFrame({
#       "min": min_value,
#       "max": max_value,
#       "median": median,
#       "mean": mean,
#       "IQ25": IQ25,
#       "IQ75": IQ75
#     })
#     stat_summary = pd.concat([pd.DataFrame({"keywords": outcome_keywords_similar_list}),
#                                 stat_summary], axis = 1)
#     return stat_summary


# print(LOSA_list)
# keywords_similarity = w2v_model.get_similar_word_2(keywords = LOSA_list)


# outcome_stat_summary = {}
# for outcome, keyword in keywords_dict.items():
#     outcome_stat_summary.setdefault(outcome, stat_summary(keyword))
# outcome_stat_summary['Gender']








'''
def create_table():
    table = pd.DataFrame(columns=['metric', 'name', 'normalized_score_small', 'normalized_score_med', 'normalized_score_large'])
    eval = [evaluate(synthetic_data[i], real_data,  aggregate=False) for i in range(3)]
    eval_small = eval[0]
    eval_med = eval[1]
    eval_large = eval[2]
    metrics_list = list(eval_small.metric)
    print(metrics_list + ['privacy'])
    table.metric = metrics_list + ['privacy']

    table.name = list(eval_small.name) + ['categorical']

    privacy_score = [CategoricalCAP.compute(
        real_data.fillna(0),
        synthetic_data[i].fillna(0),
        key_fields=['TEXT'],
        sensitive_fields=['GENDER']
        ) for i in range(len(synthetic_data))]
    table.normalized_score_small = list(eval_small.normalized_score) + [privacy_score[0]]
    table.normalized_score_med = list(eval_med.normalized_score) + [privacy_score[1]]
    table.normalized_score_large = list(eval_large.normalized_score) + [privacy_score[2]]
    return table
'''


'''
model = GaussianCopula()
real_data_0 = real_data.iloc[0:50]
# real_data_1 = real_data.iloc[51:100]
# real_data_2 = real_data.iloc[101:150]
real_data = real_data_0
# real_data
model.fit(real_data)
synthetic_data_0 = model.sample()
# model.fit(real_data_1)
synthetic_data_1 = model.sample()
# model.fit(real_data_2)
synthetic_data_2 = model.sample()
synthetic_data = [synthetic_data_0, synthetic_data_1, synthetic_data_2]
# metrics_list = ['CSTest', 'KSTest']
table = create_table()
'''


pip install sdv


pip install sdmetrics


pip install sdv


pip install pomegranate


import random
import time
from operator import attrgetter
import pandas as pd
from sdv.metrics.tabular import *

import warnings

warnings.filterwarnings("ignore")
pd.set_option('display.max_colwidth', -1)
pd.set_option('display.max_columns', None)
other_metrics_dict = {
    #'BNLikelihood': BNLikelihood, # pomegranate problem might success with python 3.9 and older
    #'BNLogLikelihood': BNLogLikelihood, # pomegranate problem might success with python 3.9 and older
    #'LogisticDetection': LogisticDetection,
    #'SVCDetection': SVCDetection,
    #'GMLogLikelihood': GMLogLikelihood,
    'CSTest': CSTest, #work this metric first
    #'KSTest': KSComplement, #work this metric first
    #'ContinuousKLDivergence': ContinuousKLDivergence,
    'DiscreteKLDivergence': DiscreteKLDivergence
}

other_metrics_dict_onlyn = {
    #'BNLikelihood': BNLikelihood, # pomegranate problem might success with python 3.9 and older
    #'BNLogLikelihood': BNLogLikelihood, # pomegranate problem might success with python 3.9 and older
    'LogisticDetection': LogisticDetection,
    'SVCDetection': SVCDetection,
    'GMLogLikelihood': GMLogLikelihood,
    #'CSTest': CSTest, #work this metric first
    'KSTest': KSComplement, #work this metric first
    'ContinuousKLDivergence': ContinuousKLDivergence,
    #'DiscreteKLDivergence': DiscreteKLDivergence
}

privacy_categorical_metrics_dict = {
    'CategoricalCAP': CategoricalCAP,
    'CategoricalZeroCAP': CategoricalZeroCAP,
    'CategoricalGeneralizedCAP': CategoricalGeneralizedCAP,
    'CategoricalKNN': CategoricalKNN,
    'CategoricalNB': CategoricalNB,
    'CategoricalRF': CategoricalRF,
    # 'CategoricalEnsemble': CategoricalEnsemble
}

privacy_numerical_metrics_dict = {
    'NumericalMLP': NumericalMLP,
    'NumericalLR': NumericalLR,
    'NumericalSVR': NumericalSVR,
    # 'NumericalRadiusNearestNeighbor': NumericalRadiusNearestNeighbor # This metrics is too slow, so we will not use.
}





def create_table(real_data, synthetic_data, numerical_columns, key_fields, sensitive_fields, target_block_size=30): #originally the block size was 100
    """
    The function to create table
    :param real_data: The original data
    :param synthetic_data: The synthetic data generate by tools, should be a List of pandas.DataFrame
    :param numerical_columns: A list of string, telling the function what columns are numerical columns
    :param key_fields: The key field that we need to calculate privacy and utility
    :param sensitive_fields: The sensitive field that we need to calculate privacy and utility. -> outcomes
    :param target_block_size: How many rows in each block, the final result will be average of all blocks
    :return: result_n(c)_avg: the numerical(categorical) result for each metrics,
    numerical(categorical)_avg: the average of all metrics, total_score: the average of numerical and categorical
    """
    #total_len = len(real_data)
    #total_len = total_len // target_block_size * target_block_size # -> why should I divide it by target_block_size?

    real_n, real_c = split_table(real_data, numerical_col=numerical_columns)

    key_fields_n, key_fields_c, sensitive_fields_n, sensitive_fields_c = get_meta_data(real_n,
                                                                                       real_c,
                                                                                       key_fields,
                                                                                       sensitive_fields)

    data_dict_n = {
        'real': real_n
    }
    data_dict_c = {
        'real': real_c
    }

    for i, syn_data in enumerate(synthetic_data):
        syn_n, syn_c = split_table(syn_data, numerical_columns)
        data_dict_n[f"level{i}"] = syn_n
        data_dict_c[f"level{i}"] = syn_c

    data_dict_n = remove_nan_in_numerical(data_dict_n)

    meta_data_n = {
        'key_fields': key_fields_n,
        'sensitive_fields': sensitive_fields_n,
    }
    meta_data_c = {
        'key_fields': key_fields_c,
        'sensitive_fields': sensitive_fields_c,
    }
    metric_list_c = privacy_categorical_metrics_dict.keys()
    metric_list_n = privacy_numerical_metrics_dict.keys()

    result_n = []
    result_c = []
    result_n_ot = []
    result_c_ot = []

    #for index in range(0, total_len, target_block_size):
    partial_n = get_part_of_data(data_dict_n, target_block_size,index = 0) # 앞에서 data_dict와 같은 역할
    partial_c = get_part_of_data(data_dict_c, target_block_size,index = 0)
    partial_real_n = partial_n['real']
    partial_real_c = partial_c['real']
    partial_n.pop('real')
    partial_c.pop('real')


    numerical = _create_table(metric_list_n, partial_real_n, partial_n, meta_data_n, mode='n')
    print('here')#-> just for debugging
    categorical = _create_table(metric_list_c, partial_real_c, partial_c, meta_data_c, mode='c')
    print('here')#-> just for debugging
    result_n.append(numerical)
    result_c.append(categorical)



    # starting for privacy
    # Assuming you have already defined other_metrics_dict, synthetic_data, and meta_data
    # Compute other metrics
    computed_metrics_otc = compute_other_metrics(other_metrics_dict, partial_real_c, partial_c)
    computed_metrics_otn = compute_other_metrics_n(other_metrics_dict_onlyn, partial_real_n, partial_n)
    result_n_ot.append(computed_metrics_otn)
    result_c_ot.append(computed_metrics_otc)

    # Print or use the computed metrics
    for metric_name, metric_value in computed_metrics_otc.items():
        print(f"{metric_name}: {metric_value}")

    for metric_name, metric_value in computed_metrics_otn.items():
        print(f"{metric_name}: {metric_value}")
    #end for privacy



    result_n_avg_ot = sum(result_n_ot) / len(result_n_ot)
    result_c_avg_ot = sum(result_c_ot) / len(result_c_ot)
    numerical_avg_ot = result_n_avg_ot.mean()
    numerical_avg_ot['time'] *= len(synthetic_data)
    categorical_avg_ot = result_c_avg_ot.mean()
    categorical_avg_ot['time'] *= len(synthetic_data)

    total_score_ot = (numerical_avg_ot + categorical_avg_ot) / 2



    result_n_avg = sum(result_n) / len(result_n)
    result_c_avg = sum(result_c) / len(result_c)
    numerical_avg = result_n_avg.mean()
    numerical_avg['time'] *= len(synthetic_data)
    categorical_avg = result_c_avg.mean()
    categorical_avg['time'] *= len(synthetic_data)

    total_score = (numerical_avg + categorical_avg) / 2

    return result_n_avg, result_c_avg, numerical_avg, categorical_avg, total_score, result_n_avg_ot, result_c_avg_ot, numerical_avg_ot, categorical_avg_ot, total_score_ot




'''
def compute_other_metrics(metrics_dict, synthetic_data, meta_data):
    results = {}
    times = {}
    for metric_name, metric_class in metrics_dict.items():
        metric_instance = metric_class()
        metric_value = metric_instance.compute(synthetic_data=synthetic_data, **meta_data)
        results[metric_name] = metric_value

    return results
'''



def compute_other_metrics(metrics_dict, _real_data, synthetic_data): # make another create_table just for privacy
    global other_metrics_dict
    # all_df = {}
    # for meta_data in meta_data_list:
    result = {}
    times = {}
    success_metrics = []
    for level, data in synthetic_data.items(): # change the name for the level
        scores = []
        for metrics in metrics_dict:
            print(metrics) # just for debugging
            _start = time.time()
            if metrics not in metrics_dict.keys():
                continue
            metric = metrics_dict[metrics]
            score = metric.compute(real_data=_real_data, synthetic_data=data)
            print('score :',score) # just for debugging
            scores.append(score)
            if metrics not in success_metrics:
                success_metrics.append(metrics)
                times[metrics] = 0
            _end = time.time() - _start
            times[metrics] += _end
        result[level] = scores
    result['time'] = [times[key] for key in success_metrics]
    df = pd.DataFrame(result, index=success_metrics)
    # df['time'] = times
    # all_df[meta_data['sensitive_fields'][0]] = df

    print('one table generated')
    return df



import time

def compute_other_metrics(other_metrics_dict, real_data, synthetic_data_dict):
    """
    Compute other metrics based on synthetic data and real data.

    :param other_metrics_dict: A dictionary of other metrics functions.
    :param real_data: The real data used as a reference for metric calculation.
    :param synthetic_data_dict: A dictionary where keys represent synthetic data levels, and values are synthetic data.
    :return: A DataFrame containing metric scores and execution times.
    """
    result = {}
    times = {}
    success_metrics = []

    for level, data in synthetic_data_dict.items():
        scores = {}
        for metric_name, metric_function in other_metrics_dict.items():
            print(metric_name)  # Just for debugging
            _start = time.time()

            # You can directly call the metric function without meta_data
            score = metric_function.compute(real_data=real_data, synthetic_data=data)

            print('score:', score)  # Just for debugging
            scores[metric_name] = score

            if metric_name not in success_metrics:
                success_metrics.append(metric_name)
                times[metric_name] = 0

            _end = time.time() - _start
            times[metric_name] += _end

        result[level] = scores

    result['time'] = [times[key] for key in success_metrics]
    df = pd.DataFrame(result, index=success_metrics)

    print('One table generated for other metrics')  # Just for debugging
    return df



def compute_other_metrics_n(metrics_dict, _real_data, synthetic_data): #
    global other_metrics_dict
    # all_df = {}
    # for meta_data in meta_data_list:
    result = {}
    times = {}
    success_metrics = []
    for level, data in synthetic_data.items(): # change the name for the level
        scores = []
        for metrics in metrics_dict:
            print(metrics) # just for debugging
            _start = time.time()
            if metrics not in metrics_dict.keys():
                continue
            metric = metrics_dict[metrics]
            score = metric.compute(real_data=_real_data, synthetic_data=data)
            print('score :',score) # just for debugging
            scores.append(score)
            if metrics not in success_metrics:
                success_metrics.append(metrics)
                times[metrics] = 0
            _end = time.time() - _start
            times[metrics] += _end
        result[level] = scores
    result['time'] = [times[key] for key in success_metrics]
    df = pd.DataFrame(result, index=success_metrics)
    # df['time'] = times
    # all_df[meta_data['sensitive_fields'][0]] = df

    print('one table generated')
    return df



def get_meta_data(real_data_n, real_data_c, key_fields, sensitive_fields):
    key_fields_c = [key for key in key_fields if key in real_data_c.columns]
    key_fields_n = [key for key in key_fields if key in real_data_n.columns]
    sensitive_fields_c = [key for key in sensitive_fields if key in real_data_c.columns]
    sensitive_fields_n = [key for key in sensitive_fields if key in real_data_n.columns]
    return key_fields_n, key_fields_c, sensitive_fields_n, sensitive_fields_c

def _create_table(metrics_list, _real_data, synthetic_data_dict: dict, meta_data, mode): #generate the privacy metrics (The colon in synthetic_data_dict: dict is a type hint in Python. It is used to specify the expected type of the synthetic_data_dict parameter.)
    global privacy_numerical_metrics_dict, privacy_categorical_metrics_dict
    # all_df = {}
    # for meta_data in meta_data_list:
    result = {}
    times = {}
    metrics_dict = privacy_numerical_metrics_dict if mode == 'n' else privacy_categorical_metrics_dict
    success_metrics = []
    for level, data in synthetic_data_dict.items(): # change the name for the level
        scores = []
        for metrics in metrics_list:
            print(metrics) # just for debugging
            _start = time.time()
            if metrics not in metrics_dict.keys():
                continue
            metric = metrics_dict[metrics]
            score = metric.compute(real_data=_real_data, synthetic_data=data, **meta_data)
            print('score :',score) # just for debugging
            scores.append(score)
            if metrics not in success_metrics:
                success_metrics.append(metrics)
                times[metrics] = 0
            _end = time.time() - _start
            times[metrics] += _end
        result[level] = scores
    result['time'] = [times[key] for key in success_metrics]
    df = pd.DataFrame(result, index=success_metrics)
    # df['time'] = times
    # all_df[meta_data['sensitive_fields'][0]] = df

    print('one table generated')
    return df


def split_table(data_table: pd.DataFrame, numerical_col=None): # for spliting the num and cat
    if numerical_col:
        numerical_data = data_table[numerical_col]
        categorical_data = data_table.drop(numerical_col, axis=1).applymap(str)
        return numerical_data, categorical_data

    dtype_list = data_table.dtypes.apply(attrgetter('kind'))
    numerical_index = []
    categorical_index = []
    for i, dtype in enumerate(dtype_list):
        if dtype == 'f' or dtype == 'i':
            numerical_index.append(i)
        else:
            categorical_index.append(i)
    return data_table.iloc[:, numerical_index], data_table.iloc[:, categorical_index]


def get_metadata(table: pd.DataFrame):
    meta_data_list = []
    clm = list(table.columns)
    # for i, column in enumerate(clm):
    #     m_d = {
    #         'key_fields': clm[:i] + clm[i + 1:],
    #         'sensitive_fields': [column]
    #     }
    #     meta_data_list.append(m_d)
    index = random.randint(0, len(clm) - 1)
    index = [1, 2]
    meta_data_list.append({
        'key_fields': [clm[i] for i in range(len(clm)) if i not in index],
        'sensitive_fields': [clm[i] for i in index]
    })
    return meta_data_list


def remove_nan_in_numerical(numerical_table_dict: dict):
    for k, v in numerical_table_dict.items():
        numerical_table_dict[k] = v.fillna(0)
    return numerical_table_dict


def get_part_of_data(table_dict, n, index): # as a result of regarding the for loop above, I fixed the index as 1. 인덱스는 시작점 역할을 하는 것 같음
    new_dict = {}
    # index = random.randint(0, 14691 - n)
    # index = 1
    for k, v in table_dict.items():
        new_dict[k] = v.iloc[index:index + n, :] #index is the start of the row and index + n is the end of the row
        new_dict[k].reset_index(drop=True, inplace=True)
    return new_dict






col_list=[]
org = pd.read_csv('gdrive/My Drive/Colab Notebooks/SOCR/dt_new_original.csv')
(org.dtypes)
columns = list(org.columns)
for col in columns:
  print(org[col].dtypes)
  if org[col].dtypes != 'object':
    col_list.append(col)
del col_list[0]
col_list


col_list


pip install --upgrade tensorflow


from keras.preprocessing.text import Tokenizer


tokenizer = Tokenizer()

tokenizer.fit_on_texts(org)
text_sequences = tokenizer.texts_to_sequences(org)


word_vocab = tokenizer.word_index


word_vocab


ob_col = []
for col in columns:
  if org[col].dtypes == 'object':
    ob_col.append(col)


ob_col


for col in ob_col:
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(col[0])
  first_obj = tokenizer.texts_to_sequences(col[0])
  if len(first_obj) > 5 :
    print(col)


for col in columns:
  print(col,org[col].unique())


org



if __name__ == '__main__':
    numerical_column = col_list
    _real_data = pd.read_csv('gdrive/My Drive/Colab Notebooks/SOCR/dt_new_original.csv')
    synthetic_data_small = pd.read_csv('gdrive/My Drive/Colab Notebooks/SOCR/dt_new_small.csv')
    synthetic_data_medium = pd.read_csv('gdrive/My Drive/Colab Notebooks/SOCR/dt_new_medium.csv')
    synthetic_data_large = pd.read_csv('gdrive/My Drive/Colab Notebooks/SOCR/dt_new_large.csv')

    data_dict = {
        'real': _real_data,
        'small': synthetic_data_small,
        'medium': synthetic_data_medium,
        'large': synthetic_data_large,
    }

    data_dict = get_part_of_data(data_dict, len(data_dict['real']), 0) # -> second hyper para is target_block_size and it decides the size of the row.

    _real_data = data_dict['real']
    data_dict.pop('real')
    data = data_dict.values()
    _key_fields = ['pain.assessment.method','pain.type','pain.cause','peep.set'] # -> should include one for each cat and num, otherwise it will show an error saying it's empty
    _sensitive_fields = ['posttib..pulses..right.', 'pupil.response.right', 'pupil.size.right', 'plateau.pressure']
    rn, rc, an, ac, t ,rn_ot, rc_ot, an_ot, ac_ot, t_ot= create_table(_real_data, data, numerical_column, _key_fields, _sensitive_fields)

    print('\n')
    print('result_n = ',rn)
    print('\n')
    print('result_c = ',rc)
    print('\n')
    print('avg_n = ',an)
    print('\n')
    print('avg_c = ',ac)
    print('\n')
    print('time = ',t)
    print('\n')

    print('result of other metrics')
    print('\n')
    print('result_n = ',rn_ot)
    print('\n')
    print('result_c = ',rc_ot)
    print('\n')
    print('avg_n = ',an_ot)
    print('\n')
    print('avg_c = ',ac_ot)
    print('\n')
    print('time = ',t_ot)
    # real_data_n, real_data_c = split_table(_real_data, numerical_col=numerical_column)
    # synthetic_data_small_n, synthetic_data_small_c = split_table(synthetic_data_small, numerical_col=numerical_column)
    # synthetic_data_medium_n, synthetic_data_medium_c = split_table(synthetic_data_medium,
    #                                                                numerical_col=numerical_column)
    # synthetic_data_large_n, synthetic_data_large_c = split_table(synthetic_data_large, numerical_col=numerical_column)

    # drop the error column
    # synthetic_data_small_n = synthetic_data_small_n.drop(columns=['bill_prov_zip_cd', 'rndrng_prov_zip_cd'])
    # synthetic_data_medium_n = synthetic_data_medium_n.drop(columns=['bill_prov_zip_cd', 'rndrng_prov_zip_cd'])
    # synthetic_data_large_n = synthetic_data_large_n.drop(columns=['rndrng_prov_zip_cd'])
    # real_data_c = real_data_c.drop(columns=['bill_prov_zip_cd', 'rndrng_prov_zip_cd'])
    # synthetic_data_large_c = synthetic_data_large_c.drop(columns=['bill_prov_zip_cd'])

    # group the tables
    # synthetic_data_c = {
    #     'real': real_data_c,
    #     'small': synthetic_data_small_c,
    #     'medium': synthetic_data_medium_c,
    #     'large': synthetic_data_large_c
    # }
    # synthetic_data_n = {
    #     'real': real_data_n,
    #     'small': synthetic_data_small_n,
    #     'medium': synthetic_data_medium_n,
    #     'large': synthetic_data_large_n
    # }
    # synthetic_data_n = remove_nan_in_numerical(synthetic_data_n)
    #
    # mc_list_c = privacy_categorical_metrics_dict.keys()
    # mc_list_n = privacy_numerical_metrics_dict.keys()
    # meta_data_list_n = get_metadata(real_data_n)
    # meta_data_list_c = get_metadata(real_data_c)
    # total_length_n = len(real_data_n)
    # total_length_c = len(real_data_c)
    # num = 100
    # partial_data_n = get_part_of_data(synthetic_data_n, num, 0)
    # partial_data_c = get_part_of_data(synthetic_data_c, num, 0)
    # start = time.time()
    #
    # preal_data_n = partial_data_n['real']
    # partial_data_n.pop('real')
    # numerical_1 = _create_table(mc_list_n, preal_data_n,
    #                             partial_data_n, meta_data_list_n[0], mode='n')
    # preal_data_c = partial_data_c['real']
    # partial_data_c.pop('real')
    # categorical_1 = _create_table(mc_list_c, preal_data_c,
    #                               partial_data_c, meta_data_list_c[0], mode='c')
    #
    # partial_data_n = get_part_of_data(synthetic_data_n, num, 200)
    # partial_data_c = get_part_of_data(synthetic_data_c, num, 200)
    #
    # preal_data_n = partial_data_n['real']
    # partial_data_n.pop('real')
    # numerical_2 = _create_table(mc_list_n, preal_data_n,
    #                             partial_data_n, meta_data_list_n[0], mode='n')
    # preal_data_c = partial_data_c['real']
    # partial_data_c.pop('real')
    # categorical_2 = _create_table(mc_list_c, preal_data_c,
    #                               partial_data_c, meta_data_list_c[0], mode='c')


%pip install python==3.9


!python --version


data_dict = {
        'real': _real_data,
        'small': synthetic_data_small,
        'medium': synthetic_data_medium,
        'large': synthetic_data_large,
    }
data_dict = get_part_of_data(data_dict, len(data_dict['real']),0) # change 40 to other nums
data_dict


#prac ver 1
def create_table(real_data, synthetic_data, numerical_columns, key_fields, sensitive_fields,  metric_list_n, metric_list_c, target_block_size=30): #originally the block size was 100
    """
    The function to create table
    :param real_data: The original data
    :param synthetic_data: The synthetic data generate by tools, should be a List of pandas.DataFrame
    :param numerical_columns: A list of string, telling the function what columns are numerical columns
    :param key_fields: The key field that we need to calculate privacy and utility
    :param sensitive_fields: The sensitive field that we need to calculate privacy and utility. -> outcomes
    :param target_block_size: How many rows in each block, the final result will be average of all blocks
    :return: result_n(c)_avg: the numerical(categorical) result for each metrics,
    numerical(categorical)_avg: the average of all metrics, total_score: the average of numerical and categorical
    """
    #total_len = len(real_data)
    #total_len = total_len // target_block_size * target_block_size # -> why should I divide it by target_block_size?

    real_n, real_c = split_table(real_data, numerical_col=numerical_columns)

    key_fields_n, key_fields_c, sensitive_fields_n, sensitive_fields_c = get_meta_data(real_n,
                                                                                       real_c,
                                                                                       key_fields,
                                                                                       sensitive_fields)

    data_dict_n = {
        'real': real_n
    }
    data_dict_c = {
        'real': real_c
    }

    for i, syn_data in enumerate(synthetic_data):
        syn_n, syn_c = split_table(syn_data, numerical_columns)
        data_dict_n[f"level{i}"] = syn_n
        data_dict_c[f"level{i}"] = syn_c

    data_dict_n = remove_nan_in_numerical(data_dict_n)

    meta_data_n = {
        'key_fields': key_fields_n,
        'sensitive_fields': sensitive_fields_n,
    }
    meta_data_c = {
        'key_fields': key_fields_c,
        'sensitive_fields': sensitive_fields_c,
    }
    #metric_list_c = privacy_categorical_metrics_dict.keys()
    #metric_list_n = privacy_numerical_metrics_dict.keys()

    result_n = []
    result_c = []

    #for index in range(0, total_len, target_block_size): # it is like a batch
    partial_n = get_part_of_data(data_dict_n, target_block_size,index = 0) # 앞에서 data_dict와 같은 역할
    partial_c = get_part_of_data(data_dict_c, target_block_size,index = 0)
    partial_real_n = partial_n['real']
    partial_real_c = partial_c['real']
    partial_n.pop('real')
    partial_c.pop('real')


    numerical = _create_table(metric_list_n, partial_real_n,
                              partial_n, meta_data_n, mode='n')
    print('here')#-> just for debugging
    categorical = _create_table(metric_list_c, partial_real_c, partial_c, meta_data_c, mode='c')
    print('here')#-> just for debugging
    result_n.append(numerical)
    result_c.append(categorical) # 왜 append 되는게 없을까?


    result_n_avg = sum(result_n) / len(result_n)
    result_c_avg = sum(result_c) / len(result_c)
    numerical_avg = result_n_avg.mean()
    numerical_avg['time'] *= len(synthetic_data)
    categorical_avg = result_c_avg.mean()
    categorical_avg['time'] *= len(synthetic_data)

    total_score = (numerical_avg + categorical_avg) / 2

    return result_n_avg, result_c_avg, numerical_avg, categorical_avg, total_score




#prac ver 1
if __name__ == '__main__':
    numerical_column = col_list
    _real_data = pd.read_csv('gdrive/My Drive/Colab Notebooks/SOCR/dt_new_original.csv')
    synthetic_data_small = pd.read_csv('gdrive/My Drive/Colab Notebooks/SOCR/dt_new_small.csv')
    synthetic_data_medium = pd.read_csv('gdrive/My Drive/Colab Notebooks/SOCR/dt_new_medium.csv')
    synthetic_data_large = pd.read_csv('gdrive/My Drive/Colab Notebooks/SOCR/dt_new_large.csv')

    data_dict = {
        'real': _real_data,
        'small': synthetic_data_small,
        'medium': synthetic_data_medium,
        'large': synthetic_data_large,
    }

    data_dict = get_part_of_data(data_dict, len(data_dict['real']), 0) # -> second hyper para is target_block_size and it decides the size of the row.

    _real_data = data_dict['real']
    data_dict.pop('real')
    data = data_dict.values()
    _key_fields = ['pain.assessment.method','pain.type','pain.cause','peep.set'] # -> should include one for each cat and num, otherwise it will show an error saying it's empty
    _sensitive_fields = ['posttib..pulses..right.', 'pupil.response.right', 'pupil.size.right','plateau.pressure']
    rn, rc, an, ac, t = create_table(_real_data, data, numerical_column, _key_fields, _sensitive_fields, {'NumericalMLP','NumericalLR','NumericalSVR'}, {'CategoricalGeneralizedCAP','CategoricalCAP','CategoricalKNN'})
    print('\n')
    print('result_n = ')
    print(rn)
    print('\n')

    print('result_c = ')
    print(rc)
    print('\n')

    print('avg_n = ')
    print(an)
    print('\n')

    print('avg_c = ')
    print(ac)
    print('\n')

    print('time = ')
    print(t)



# This have to be done for checking whether result is proper or not. WOn't be run till everything is done.
from sdv.evaluation.single_table import run_diagnostic

diagnostic_report = run_diagnostic(
    real_data=real_data,
    synthetic_data=synthetic_data,
    metadata=metadata)



